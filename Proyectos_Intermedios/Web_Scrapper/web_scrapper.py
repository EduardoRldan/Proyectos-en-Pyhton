import requests
from bs4 import BeautifulSoup
import csv
import time

# Funci√≥n para extraer datos de Hacker News
def obtener_noticias(paginas=1):
    base_url = "https://news.ycombinator.com/news?p="
    headers = {"User-Agent": "Mozilla/5.0"}
    noticias = []

    for pagina in range(1, paginas + 1):
        print(f"üì° Extrayendo datos de la p√°gina {pagina}...")
        try:
            respuesta = requests.get(base_url + str(pagina), headers=headers, timeout=10)
            respuesta.raise_for_status()
            soup = BeautifulSoup(respuesta.text, "html.parser")

            # Extraer t√≠tulos, enlaces, votos y comentarios
            titulos = soup.find_all("a", class_="storylink")
            subtextos = soup.find_all("td", class_="subtext")

            for i in range(len(titulos)):
                titulo = titulos[i].text
                enlace = titulos[i]["href"]
                
                # Extraer n√∫mero de votos (upvotes)
                votos_tag = subtextos[i].find("span", class_="score")
                votos = int(votos_tag.text.replace(" points", "")) if votos_tag else 0
                
                # Extraer n√∫mero de comentarios
                comentarios_tag = subtextos[i].find_all("a")[-1]
                comentarios_texto = comentarios_tag.text
                comentarios = int(comentarios_texto.split()[0]) if "comment" in comentarios_texto else 0

                noticias.append([titulo, enlace, votos, comentarios])

        except requests.exceptions.RequestException as e:
            print(f"‚ùå Error al obtener la p√°gina {pagina}: {e}")

        time.sleep(2)  # Espera 2 segundos entre cada p√°gina para evitar bloqueos

    return noticias

# Funci√≥n para guardar en CSV
def guardar_csv(noticias, nombre_archivo="noticias_hacker_news.csv"):
    with open(nombre_archivo, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["T√≠tulo", "Enlace", "Votos", "Comentarios"])
        writer.writerows(noticias)
    print(f"‚úÖ Datos guardados en '{nombre_archivo}'")

# Ejecutar el scraper en las primeras 3 p√°ginas
noticias_extraidas = obtener_noticias(paginas=3)
guardar_csv(noticias_extraidas)
